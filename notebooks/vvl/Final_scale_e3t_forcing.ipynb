{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in e3t and create a +/-2 m SSH versions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.parser import parse\n",
    "import os\n",
    "from netCDF4 import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_begin = parse('5 june 2015')\n",
    "date_end   = parse('12 june 2015')\n",
    "path       = '/results2/SalishSea/nowcast-green.201806/'\n",
    "filetype   = 'carp_T'\n",
    "depth_change = 2\n",
    "out_e3t_frac = '/home/rmueller/Projects/MIDOSS/analysis-rachael/notebooks/vvl/e3t_frac_dz_2.nc'\n",
    "\n",
    "# pick Salmon Bank location [256,265], but remember that MOHID is transposed! such that SSC [yloc_ssc,xloc_ssc]-> [xloc_ssc,yloc_ssc]_mohid = [yloc_mohid,xloc_mohid]\n",
    "yloc_mohid       = 265\n",
    "xloc_mohid       = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mung_array(SSC_gridded_array, array_slice_type):\n",
    "    \"\"\"Transform an array containing SalishSeaCast-gridded data and transform it \n",
    "       into a MOHID-gridded array by:\n",
    "         1) Cutting off the grid edges\n",
    "         2) Transposing the X and Y axes\n",
    "         3) Flipping the depth dimension, if it is present\n",
    "         4) Converting the NaNs to 0\n",
    "    \n",
    "        :arg SSC_gridded_array: SalishSeaCast-gridded array\n",
    "        :type numpy.ndarray: :py:class:'ndarray'\n",
    "    \n",
    "        :arg array_slice_type: str, one of '2D' or '3D'\n",
    "        :type str: :py:class:'str'\n",
    "\n",
    "        :return MOHID_gridded_array: MOHID-gridded array produced by applying operation\n",
    "                                     1-4 on SSC_gridded_array\n",
    "        :type numpy.ndarray: :py:class:'ndarray'\n",
    "    \"\"\"\n",
    "    shape = SSC_gridded_array.shape\n",
    "    ndims = len(shape)\n",
    "    assert(array_slice_type in  ('2D', '3D')), f\"Invalid option {array_slice_type}. array_slice_type must be one of ('2D', '3D')\"\n",
    "    if array_slice_type is '2D':\n",
    "        assert(ndims in (2,3)), f'The shape of the array given is {shape}, while the option chosen was {array_slice_type}'\n",
    "        if ndims == 2:\n",
    "            MOHID_gridded_array = SSC_gridded_array[1:897:,1:397]\n",
    "            del(SSC_gridded_array)\n",
    "            MOHID_gridded_array = np.transpose(MOHID_gridded_array, [1,0])\n",
    "        else:\n",
    "            MOHID_gridded_array = SSC_gridded_array[...,1:897:,1:397]\n",
    "            del(SSC_gridded_array)\n",
    "            MOHID_gridded_array = np.transpose(MOHID_gridded_array, [0,2,1])\n",
    "\n",
    "    else:\n",
    "        assert(ndims in (3,4)), f'The shape of the array given is {shape}, while the option chosen was {array_slice_type}'\n",
    "        MOHID_gridded_array = SSC_gridded_array[...,1:897:,1:397]\n",
    "        del(SSC_gridded_array)\n",
    "        if ndims == 3:\n",
    "            MOHID_gridded_array = np.transpose(MOHID_gridded_array, [0,2,1])\n",
    "            MOHID_gridded_array = np.flip(MOHID_gridded_array, axis = 0)\n",
    "        else:\n",
    "            MOHID_gridded_array = np.transpose(MOHID_gridded_array, [0,1,3,2])\n",
    "            MOHID_gridded_array = np.flip(MOHID_gridded_array, axis = 1)\n",
    "\n",
    "    MOHID_gridded_array = np.nan_to_num(MOHID_gridded_array).astype('float64')\n",
    "\n",
    "    return MOHID_gridded_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ashu's function for writing HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_grid(data, datearrays, metadata, filename, groupname, accumulator, compression_level):\n",
    "    shape = data[0].shape\n",
    "    with h5py.File(filename) as f:\n",
    "        time_group = f.get('/Time')\n",
    "        if time_group is None:\n",
    "            time_group = f.create_group('/Time')\n",
    "        data_group_path = f'/Results/{groupname}'\n",
    "        data_group = f.get(data_group_path)\n",
    "        if data_group is None:\n",
    "            data_group = f.create_group(data_group_path)\n",
    "\n",
    "        for i, datearray in enumerate(datearrays):\n",
    "            numeric_attribute = ((5 - len(str(i + accumulator))) * '0') + str(i + accumulator)\n",
    "            child_name = 'Time_' + numeric_attribute\n",
    "            timestamp = time_group.get(child_name)\n",
    "            if timestamp is None:\n",
    "                dataset = time_group.create_dataset(\n",
    "                    child_name,\n",
    "                    shape = (6,),\n",
    "                    data = datearray,\n",
    "                    chunks = (6,),\n",
    "                    compression = 'gzip',\n",
    "                    compression_opts = compression_level\n",
    "                    )\n",
    "                time_metadata = {\n",
    "                    'Maximum' : np.array(datearray[0]),\n",
    "                    'Minimum' : np.array([-0.]),\n",
    "                    'Units' : b'YYYY/MM/DD HH:MM:SS'\n",
    "                    }\n",
    "                dataset.attrs.update(time_metadata)\n",
    "            else:\n",
    "                assert (np.asarray(timestamp) == datearray).all(), f'Time record {child_name} exists and does not match with {datearray}'\n",
    "\n",
    "            child_name = groupname + '_' + numeric_attribute\n",
    "            if data_group.get(child_name) is not None:\n",
    "                print(f'Dataset already exists at {child_name}')\n",
    "            else:\n",
    "                dataset = data_group.create_dataset(\n",
    "                    child_name,\n",
    "                    shape = shape,\n",
    "                    data = data[i],\n",
    "                    chunks = shape,\n",
    "                    compression = 'gzip',\n",
    "                    compression_opts = compression_level\n",
    "                    )\n",
    "                dataset.attrs.update(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate list of dates from user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "daterange = [date_begin, date_end]\n",
    "# append all filename strings within daterange to lists\n",
    "e3t_list = []\n",
    "for day in range(np.diff(daterange)[0].days + 1):\n",
    "    datestamp = daterange[0] + timedelta(days = day)\n",
    "    datestr1 = datestamp.strftime('%d%b%y').lower()\n",
    "    datestr2 = datestamp.strftime('%Y%m%d')\n",
    "\n",
    "    # check if file exists. exit if it does not. add path to list if it does.\n",
    "    file_path = f'{path}{datestr1}/SalishSea_1h_{datestr2}_{datestr2}_{filetype}.nc'\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f'File {file_path} not found. Check Directory and/or Date Range.')\n",
    "    e3t_list.append(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/results2/SalishSea/nowcast-green.201806/05jun15/SalishSea_1h_20150605_20150605_carp_T.nc',\n",
       " '/results2/SalishSea/nowcast-green.201806/06jun15/SalishSea_1h_20150606_20150606_carp_T.nc',\n",
       " '/results2/SalishSea/nowcast-green.201806/07jun15/SalishSea_1h_20150607_20150607_carp_T.nc',\n",
       " '/results2/SalishSea/nowcast-green.201806/08jun15/SalishSea_1h_20150608_20150608_carp_T.nc',\n",
       " '/results2/SalishSea/nowcast-green.201806/09jun15/SalishSea_1h_20150609_20150609_carp_T.nc',\n",
       " '/results2/SalishSea/nowcast-green.201806/10jun15/SalishSea_1h_20150610_20150610_carp_T.nc',\n",
       " '/results2/SalishSea/nowcast-green.201806/11jun15/SalishSea_1h_20150611_20150611_carp_T.nc',\n",
       " '/results2/SalishSea/nowcast-green.201806/12jun15/SalishSea_1h_20150612_20150612_carp_T.nc']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e3t_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = mung_array(xr.open_dataset('https://salishsea.eos.ubc.ca/erddap/griddap/ubcSSn3DMeshMaskV17-02').isel(time = 0).tmask.values, '3D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test process with one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = xr.open_dataset(e3t_list[0])\n",
    "datetimelist = data.time_counter.values.astype('datetime64[s]').astype(datetime)\n",
    "datearrays = [np.array(\n",
    "        [d.year, d.month, d.day, d.hour, d.minute,d.second]\n",
    "        ).astype('float64') for d in datetimelist]\n",
    "del(datetimelist)\n",
    "e3t = data.e3t.values\n",
    "e3t = mung_array(e3t, '3D')\n",
    "e3t = e3t*mask\n",
    "metadata = {\n",
    "   'FillValue' : np.array([0.]),\n",
    "   'Units' : b'?C'\n",
    "   }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a matrix of %depth values for all locations and times by looping through time and space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading e3t_frac_dz from file\n"
     ]
    }
   ],
   "source": [
    "total_depth = e3t.sum(1)\n",
    "e3t_frac_dz = np.empty_like(e3t)\n",
    "\n",
    "if os.path.isfile('/home/rmueller/data/vvl/test_e3t_frac.nc'): \n",
    "    test = xr.open_dataset('/home/rmueller/data/vvl/test_e3t_frac.nc')\n",
    "    print('Loading e3t_frac_dz from file')\n",
    "else:\n",
    "    print('Creating matrix of percent total depth for e3t levels (this will take some time)')\n",
    "    for t in range(e3t.shape[0]):\n",
    "        for i in range(e3t.shape[2]):\n",
    "            for j in range(e3t.shape[3]):\n",
    "                for z in range(e3t.shape[1]):\n",
    "                    e3t_frac_dz[t,z,i,j] = e3t[t,z,i,j]/total_depth[t,i,j]\n",
    "                    \n",
    "    print('saving to ', out_e3t_frac)\n",
    "    # convert to xarray for ease of output\n",
    "    xrfrac = xr.DataArray(e3t_frac_dz)\n",
    "    xrfrac.to_netcdf('/home/rmueller/data/vvl/test_e3t_frac.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate new e3t based on desired depth change\n",
    "e3t_new = (total_depth[1,256,265] + depth_change) * e3t_frac_dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 40, 396, 896)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e3t_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test process with multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in e3t_list:\n",
    "    data = xr.open_dataset(file_path)\n",
    "    datetimelist = data.time_counter.values.astype('datetime64[s]').astype(datetime)\n",
    "    datearrays = [np.array(\n",
    "            [d.year, d.month, d.day, d.hour, d.minute,d.second]\n",
    "            ).astype('float64') for d in datetimelist]\n",
    "    del(datetimelist)\n",
    "    e3t = data.e3t.values\n",
    "    e3t = mung_array(e3t, '3D')\n",
    "    e3t = e3t*mask\n",
    "    metadata = {\n",
    "       'FillValue' : np.array([0.]),\n",
    "       'Units' : b'?C'\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 40, 396, 896)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e3t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
