{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in e3t and create a +/-2 m SSH versions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.parser import parse\n",
    "import os\n",
    "from netCDF4 import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_begin = parse('5 june 2015')\n",
    "date_end   = parse('12 june 2015')\n",
    "path       = '/results2/SalishSea/nowcast-green.201806/'\n",
    "filetype   = 'carp_T'\n",
    "depth_change = 2\n",
    "out_e3t_frac = '/home/rmueller/Projects/MIDOSS/analysis-rachael/notebooks/vvl/e3t_frac_dz_2.nc'\n",
    "\n",
    "# pick Salmon Bank location [256,265], but remember that MOHID is transposed! such that SSC [yloc_ssc,xloc_ssc]-> [xloc_ssc,yloc_ssc]_mohid = [yloc_mohid,xloc_mohid]\n",
    "yloc_mohid       = 265\n",
    "xloc_mohid       = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ashu's function to transpose matrices to MOHID orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mung_array(SSC_gridded_array, array_slice_type):\n",
    "    \"\"\"Transform an array containing SalishSeaCast-gridded data and transform it \n",
    "       into a MOHID-gridded array by:\n",
    "         1) Cutting off the grid edges\n",
    "         2) Transposing the X and Y axes\n",
    "         3) Flipping the depth dimension, if it is present\n",
    "         4) Converting the NaNs to 0\n",
    "    \n",
    "        :arg SSC_gridded_array: SalishSeaCast-gridded array\n",
    "        :type numpy.ndarray: :py:class:'ndarray'\n",
    "    \n",
    "        :arg array_slice_type: str, one of '2D' or '3D'\n",
    "        :type str: :py:class:'str'\n",
    "\n",
    "        :return MOHID_gridded_array: MOHID-gridded array produced by applying operation\n",
    "                                     1-4 on SSC_gridded_array\n",
    "        :type numpy.ndarray: :py:class:'ndarray'\n",
    "    \"\"\"\n",
    "    shape = SSC_gridded_array.shape\n",
    "    ndims = len(shape)\n",
    "    assert(array_slice_type in  ('2D', '3D')), f\"Invalid option {array_slice_type}. array_slice_type must be one of ('2D', '3D')\"\n",
    "    if array_slice_type is '2D':\n",
    "        assert(ndims in (2,3)), f'The shape of the array given is {shape}, while the option chosen was {array_slice_type}'\n",
    "        if ndims == 2:\n",
    "            MOHID_gridded_array = SSC_gridded_array[1:897:,1:397]\n",
    "            del(SSC_gridded_array)\n",
    "            MOHID_gridded_array = np.transpose(MOHID_gridded_array, [1,0])\n",
    "        else:\n",
    "            MOHID_gridded_array = SSC_gridded_array[...,1:897:,1:397]\n",
    "            del(SSC_gridded_array)\n",
    "            MOHID_gridded_array = np.transpose(MOHID_gridded_array, [0,2,1])\n",
    "\n",
    "    else:\n",
    "        assert(ndims in (3,4)), f'The shape of the array given is {shape}, while the option chosen was {array_slice_type}'\n",
    "        MOHID_gridded_array = SSC_gridded_array[...,1:897:,1:397]\n",
    "        del(SSC_gridded_array)\n",
    "        if ndims == 3:\n",
    "            MOHID_gridded_array = np.transpose(MOHID_gridded_array, [0,2,1])\n",
    "            MOHID_gridded_array = np.flip(MOHID_gridded_array, axis = 0)\n",
    "        else:\n",
    "            MOHID_gridded_array = np.transpose(MOHID_gridded_array, [0,1,3,2])\n",
    "            MOHID_gridded_array = np.flip(MOHID_gridded_array, axis = 1)\n",
    "\n",
    "    MOHID_gridded_array = np.nan_to_num(MOHID_gridded_array).astype('float64')\n",
    "\n",
    "    return MOHID_gridded_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ashu's function for writing HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_grid(data, datearrays, metadata, filename, groupname, accumulator, compression_level):\n",
    "    shape = data[0].shape\n",
    "    with h5py.File(filename) as f:\n",
    "        time_group = f.get('/Time')\n",
    "        if time_group is None:\n",
    "            time_group = f.create_group('/Time')\n",
    "        data_group_path = f'/Results/{groupname}'\n",
    "        data_group = f.get(data_group_path)\n",
    "        if data_group is None:\n",
    "            data_group = f.create_group(data_group_path)\n",
    "\n",
    "        for i, datearray in enumerate(datearrays):\n",
    "            numeric_attribute = ((5 - len(str(i + accumulator))) * '0') + str(i + accumulator)\n",
    "            child_name = 'Time_' + numeric_attribute\n",
    "            timestamp = time_group.get(child_name)\n",
    "            if timestamp is None:\n",
    "                dataset = time_group.create_dataset(\n",
    "                    child_name,\n",
    "                    shape = (6,),\n",
    "                    data = datearray,\n",
    "                    chunks = (6,),\n",
    "                    compression = 'gzip',\n",
    "                    compression_opts = compression_level\n",
    "                    )\n",
    "                time_metadata = {\n",
    "                    'Maximum' : np.array(datearray[0]),\n",
    "                    'Minimum' : np.array([-0.]),\n",
    "                    'Units' : b'YYYY/MM/DD HH:MM:SS'\n",
    "                    }\n",
    "                dataset.attrs.update(time_metadata)\n",
    "            else:\n",
    "                assert (np.asarray(timestamp) == datearray).all(), f'Time record {child_name} exists and does not match with {datearray}'\n",
    "\n",
    "            child_name = groupname + '_' + numeric_attribute\n",
    "            if data_group.get(child_name) is not None:\n",
    "                print(f'Dataset already exists at {child_name}')\n",
    "            else:\n",
    "                dataset = data_group.create_dataset(\n",
    "                    child_name,\n",
    "                    shape = shape,\n",
    "                    data = data[i],\n",
    "                    chunks = shape,\n",
    "                    compression = 'gzip',\n",
    "                    compression_opts = compression_level\n",
    "                    )\n",
    "                dataset.attrs.update(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate list of dates from user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "daterange = [date_begin, date_end]\n",
    "# append all filename strings within daterange to lists\n",
    "e3t_list = []\n",
    "for day in range(np.diff(daterange)[0].days + 1):\n",
    "    datestamp = daterange[0] + timedelta(days = day)\n",
    "    datestr1 = datestamp.strftime('%d%b%y').lower()\n",
    "    datestr2 = datestamp.strftime('%Y%m%d')\n",
    "\n",
    "    # check if file exists. exit if it does not. add path to list if it does.\n",
    "    file_path = f'{path}{datestr1}/SalishSea_1h_{datestr2}_{datestr2}_{filetype}.nc'\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f'File {file_path} not found. Check Directory and/or Date Range.')\n",
    "    e3t_list.append(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/results2/SalishSea/nowcast-green.201806/05jun15/SalishSea_1h_20150605_20150605_carp_T.nc',\n",
       " '/results2/SalishSea/nowcast-green.201806/06jun15/SalishSea_1h_20150606_20150606_carp_T.nc',\n",
       " '/results2/SalishSea/nowcast-green.201806/07jun15/SalishSea_1h_20150607_20150607_carp_T.nc',\n",
       " '/results2/SalishSea/nowcast-green.201806/08jun15/SalishSea_1h_20150608_20150608_carp_T.nc',\n",
       " '/results2/SalishSea/nowcast-green.201806/09jun15/SalishSea_1h_20150609_20150609_carp_T.nc',\n",
       " '/results2/SalishSea/nowcast-green.201806/10jun15/SalishSea_1h_20150610_20150610_carp_T.nc',\n",
       " '/results2/SalishSea/nowcast-green.201806/11jun15/SalishSea_1h_20150611_20150611_carp_T.nc',\n",
       " '/results2/SalishSea/nowcast-green.201806/12jun15/SalishSea_1h_20150612_20150612_carp_T.nc']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e3t_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = mung_array(xr.open_dataset('https://salishsea.eos.ubc.ca/erddap/griddap/ubcSSn3DMeshMaskV17-02').isel(time = 0).tmask.values, '3D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 396, 896)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test process with one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = xr.open_dataset(e3t_list[0])\n",
    "datetimelist = data.time_counter.values.astype('datetime64[s]').astype(datetime)\n",
    "datearrays = [np.array(\n",
    "        [d.year, d.month, d.day, d.hour, d.minute,d.second]\n",
    "        ).astype('float64') for d in datetimelist]\n",
    "del(datetimelist)\n",
    "e3t = data.e3t.values\n",
    "e3t = mung_array(e3t, '3D')\n",
    "e3t = e3t*mask\n",
    "metadata = {\n",
    "   'FillValue' : np.array([0.]),\n",
    "   'Units' : b'?C'\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 40, 396, 896)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e3t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 396, 896)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_depth = e3t.sum(1)\n",
    "total_depth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = total_depth[1,1,1]\n",
    "if tmp != 0:\n",
    "    print(\"non-zero\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac = np.empty_like(e3t)\n",
    "for z in range(e3t.shape[1]):\n",
    "    frac[1,z,256,265] = e3t[1,z,256,265]/total_depth[1,256,265]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.18629024,  0.18699895,\n",
       "        0.15091983,  0.11079468,  0.07471557,  0.04797064,  0.03084826,\n",
       "        0.02089261,  0.0154253 ,  0.01251671,  0.01099547,  0.01020691,\n",
       "        0.00980005,  0.00959063,  0.00948297,  0.00942766,  0.00939925,\n",
       "        0.00938466,  0.00937717,  0.00937332,  0.00937135,  0.00937034,\n",
       "        0.00936982,  0.00936955,  0.00936941,  0.00936934,  0.0093693 ])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frac[1,:,256,265]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "e3t_new = total_depth[1,256,265] * frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 40, 396, 896)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e3t_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(e3t_new[1,:,256,265] - e3t[1,:,256,265])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now try with adding 2m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.37258048  0.37399789  0.30183966  0.22158936  0.14943115  0.09594128\n",
      "  0.06169653  0.04178523  0.03085061  0.02503342  0.02199094  0.02041383\n",
      "  0.0196001   0.01918126  0.01896594  0.01885531  0.0187985   0.01876932\n",
      "  0.01875434  0.01874665  0.0187427   0.01874067  0.01873963  0.0187391\n",
      "  0.01873882  0.01873868  0.01873861]\n"
     ]
    }
   ],
   "source": [
    "for z in range(e3t.shape[1]):\n",
    "    frac[1,z,256,265] = e3t[1,z,256,265]/(total_depth[1,256,265])\n",
    "\n",
    "e3t_new = (total_depth[1,256,265] + 2) * frac\n",
    "# print difference between e3t adjusted up by 2m and original e3t\n",
    "print(e3t_new[1,:,256,265] - e3t[1,:,256,265])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now loop through time and space to get scale values for all locations and times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rmueller/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "total_depth = e3t.sum(1)\n",
    "e3t_frac_dz = np.empty_like(e3t)\n",
    "\n",
    "for t in range(e3t.shape[0]):\n",
    "    for i in range(e3t.shape[2]):\n",
    "        for j in range(e3t.shape[3]):\n",
    "            for z in range(e3t.shape[1]):\n",
    "                e3t_frac_dz[t,z,i,j] = e3t[t,z,i,j]/total_depth[t,i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to  /home/rmueller/Projects/MIDOSS/analysis-rachael/notebooks/vvl/e3t_frac_dz_2.nc\n"
     ]
    }
   ],
   "source": [
    "print('saving to ', out_e3t_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrfrac = xr.DataArray(e3t_frac_dz)\n",
    "xrfrac.to_netcdf('test_e3t_frac.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: b'/home/rmueller/Projects/MIDOSS/analysis-rachael/notebooks/vvl/e3t_frac_dz_2.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-7a2ba4f1fada>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mncout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_e3t_frac\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'NETCDF4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mncout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lon'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me3t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;31m# I'm not sure this is right\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mncout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me3t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0;31m# lat/lon might be swapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mncout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'depth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me3t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mncout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me3t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: b'/home/rmueller/Projects/MIDOSS/analysis-rachael/notebooks/vvl/e3t_frac_dz_2.nc'"
     ]
    }
   ],
   "source": [
    "ncout = Dataset(out_e3t_frac,'w','NETCDF4');\n",
    "ncout.createDimension('lon', e3t.shape[3]); # I'm not sure this is right\n",
    "ncout.createDimension('lat', e3t.shape[2]);  # lat/lon might be swapped\n",
    "ncout.createDimension('depth', e3t.shape[1])\n",
    "ncout.createDimension('time', e3t.shape[0]);\n",
    "e3tvar = ncout.createVariable('e3t','float32',('time','depth','lat','lon')); e3tvar.setncattr('units','mm'); e3tvar[:] = e3t_frac_dz;\n",
    "ncout.close();\n",
    "#e3t_frac_dz.to_netcdf(path=out_e3t_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate new e3t based on desired depth change\n",
    "e3t_new = (total_depth[1,256,265] + depth_change) * e3t_frac_dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale process to calculate new e3t for all days in run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function call: process_grid(e3t_list, 'e3t', dirname+e3t, 'vvl', compression_level)\n",
    "# def process_grid(file_paths, datatype, filename, groupname, compression_level, weighting_matrix_obj=None)\n",
    "for file_path in e3t_list:\n",
    "    data = xr.open_dataset(file_path)\n",
    "    datetimelist = data.time_counter.values.astype('datetime64[s]').astype(datetime)\n",
    "    datearrays = [np.array(\n",
    "        [d.year, d.month, d.day, d.hour, d.minute,d.second]\n",
    "        ).astype('float64') for d in datetimelist]\n",
    "    del(datetimelist)\n",
    "    data = data.e3t.values\n",
    "    e3t = mung_array(data, '3D')\n",
    "    e3t = e3t*mask\n",
    "    metadata = {\n",
    "       'FillValue' : np.array([0.]),\n",
    "       'Units' : b'?C'\n",
    "       }\n",
    "    e3t_new = (total_depth[1,256,265] + depth_change) * e3t_frac_dz\n",
    "    \n",
    "    # need to fill out datearrays, metadata, filename, groupname, accumulator, compression_level\n",
    "    write_grid(e3t_new, datearrays, metadata, filename, groupname, accumulator, compression_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 40, 898, 398)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3D mask truncated and transposed at needed by MOHID\n",
    "mask_3D = xr.open_dataset('https://salishsea.eos.ubc.ca/erddap/griddap/ubcSSn3DMeshMaskV17-02').tmask.isel(time=0).values[...,1:897, 1:397]\n",
    "mask_3D = np.transpose(mask_3D, [0,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2015, 6, 5, 0, 0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e3t_list = forcing_paths.salishseacast_paths(date_begin, date_end, salishseacast_path, 'carp_T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
